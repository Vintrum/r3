{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Akhil\\Documents\\cs688\\Proj\\m3\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import set_seed\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "HF_HUB_DISABLE_IMPLICIT_TOKEN=1\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ans_entroy(question, facts):\n",
    "    input_text = \"\"\n",
    "    for fact in facts:\n",
    "        if input_text == \"\":\n",
    "            input_text =  fact\n",
    "        else:\n",
    "            input_text =  input_text  + ', ' + fact \n",
    "    input_text = \"Given fact: \" + input_text + ', ' + question + '\\nAnswer:'\n",
    "    prom_text = input_text\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_ids = tokenizer.encode(prom_text, return_tensors=\"pt\").to(device)\n",
    "        beam0_logits = model(input_ids).logits[:, -1, :].to(device)\n",
    "        beam0_prob = torch.softmax(beam0_logits, dim=-1).to(device)\n",
    "        ans_h = -(beam0_prob * torch.log(beam0_prob)).sum().to(device)\n",
    "    if len(facts)>3:\n",
    "        ans_h=ans_h/2\n",
    "    return ans_h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=pd.read_csv(r\"one_hop_test.csv\",index_col=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('mp2.csv', mode='w', newline='',encoding=\"UTF-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write header\n",
    "    writer.writerow(['Query', 'Labels','Entity_set','Evidence','types'])\n",
    "    \n",
    "    # Write data\n",
    "    for key, value in df.items():\n",
    "        writer.writerow([key, value['Label'],value[\"Entity_set\"],value['Evidence'],value['types']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Open the pickle file in read-binary mode\n",
    "with open(r'factkg\\factkg_train.pickle', 'rb') as file:\n",
    "    # Load the object from the file\n",
    "    df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"mp3.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.head(1519)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qu = list(df[\"Query\"][:])\n",
    "questions_dict={}\n",
    "entity_set_dict = {}\n",
    "label_set_dict = {}\n",
    "evidence_set_dict = {}\n",
    "q_id=0\n",
    "for i in range(len(df)):\n",
    "    questions_dict[q_id]=qu[q_id]  \n",
    "    label_set_dict[q_id] = df[\"Labels\"][q_id]\n",
    "    t1=literal_eval(df[\"Entity_set\"][q_id])\n",
    "    t2=literal_eval(df[\"Evidence\"][q_id])\n",
    "    if len(t1)>1:\n",
    "        entity_set_dict[q_id] =t1[0]\n",
    "        evidence_set_dict[q_id]=t2[t1[0]][0][0]\n",
    "    else:\n",
    "        entity_set_dict[q_id] =t1[0]\n",
    "        evidence_set_dict[q_id]=t2[t1[0]][0][0]\n",
    "\n",
    "    q_id+=1\n",
    "\n",
    "\n",
    "df_len=len(questions_dict)\n",
    "a=0\n",
    "data_num=range(a,df_len)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = CrossEncoder(\"cross-encoder/stsb-distilroberta-base\",activation_fn=torch.nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cos_sim(query, corpus):\n",
    "\n",
    "    ranks = model2.rank(query, corpus,return_documents=True)\n",
    "    stop=0\n",
    "    result=[]\n",
    "    scores=[]\n",
    "    for i in ranks:\n",
    "\n",
    "        triplet=literal_eval(i['text'])\n",
    "        result.append(triplet)\n",
    "        scores.append(i['score'])\n",
    "        # stop+=1\n",
    "        # if stop==5:\n",
    "        #     break\n",
    "\n",
    "    return(result, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x00000218E9184FA0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Akhil\\Documents\\cs688\\Proj\\m3\\.venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "with open('dbpedia_2015_undirected_light.pickle', 'rb') as f:\n",
    "    kg = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1519 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Planet/orbitalPeriod', '\"1202.846\"'], ['epoch', '\"2005-11-26\"'], ['orbitalPeriod', '\"1.039258944E8\"'], ['rotationPeriod', '\"17.28\"']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "k1=[]\n",
    "hits_1=0\n",
    "hits_3=0\n",
    "hits_5=0\n",
    "sub=0    \n",
    "q_a_pairs={}\n",
    "e_set=list(entity_set_dict.values())\n",
    "for ii in tqdm(data_num):\n",
    "    question = questions_dict[ii]\n",
    "    entity = e_set[ii]\n",
    "    \n",
    "    # print(question, entity_set, ground_truth)\n",
    "    # I have heard that Mobyland had a successor. ['Mobyland'] [True]\n",
    "    \n",
    "    triplets=[]\n",
    "    cossim=[]\n",
    "    i=0\n",
    "    pos_list=[]\n",
    "    # question_input = tokenizer([question], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    # question_embedding = question_model(**question_input).last_hidden_state.mean(dim=1)\n",
    "    # question_embedding2 = question_model2(**question_input).last_hidden_state.mean(dim=1)\n",
    "    \n",
    "    subgraph = []\n",
    "    super_graph=[]\n",
    "    hop_2=[]\n",
    "    hop_3=[]\n",
    "    if entity in kg:\n",
    "        \n",
    "    \n",
    "        # {'foundingYear': ['\"2006\"'], 'type': ['Private_company_limited_by_shares'], 'locationCity': ['Warsaw'], \n",
    "        # 'locationCountry': ['Poland'], 'regionServed': ['Poland'], 'keyPerson': ['Adam_Kuriański', 'Andrzej_Chajec'],\n",
    "        # 'industry': ['\"Telecommunications\"', 'Telecommunication'], 'service': ['Telecommunication'], 'successor': ['\"Aero 2\"'], \n",
    "        # 'keyPeople': ['Adam_Kuriański', '\"\\'\\'\\'\"', 'Andrzej_Chajec']}\n",
    "\n",
    "        for relation, object in kg[entity].items():\n",
    "            if [relation] not in subgraph:\n",
    "                h1=object[0]\n",
    "                s1=[entity,relation,object[0]]\n",
    "                subgraph.append(str(s1))\n",
    "                # print('step1')\n",
    "        r2,scores=calc_cos_sim(question,subgraph)\n",
    "        if len(subgraph)==0:\n",
    "            sub+=1\n",
    "            continue\n",
    "        else:\n",
    "            ev_list=evidence_set_dict[ii]\n",
    "            \n",
    "        # r2=calc_freq_sim(qu[i],list(e_list[0]))\n",
    "        r2_top1=r2[0]\n",
    "\n",
    "        subgraph=r2[:4]\n",
    "\n",
    "        for iter in subgraph:\n",
    "            for relation, object in kg[iter[2]].items():\n",
    "                if [relation] not in subgraph:\n",
    "                    h2=object[0]\n",
    "                    s2=[s1,entity,relation,object[0]]\n",
    "                    hop_2.append(str([relation,object[0]]))\n",
    "        r2,scores=calc_cos_sim(question,hop_2)\n",
    "        r2_top1=r2[0]\n",
    "\n",
    "        subgraph=r2[:4]\n",
    "        # print(subgraph)\n",
    "        # for iter in subgraph:\n",
    "        #             for relation, object in kg[iter[1]].items():\n",
    "        #                 if [relation] not in subgraph:\n",
    "        #                     h2=object[0]\n",
    "        #                     s2=[s1,entity,relation,object[0]]\n",
    "        #                     hop_3.append(str([relation,object[0]]))\n",
    "        # r2,scores=calc_cos_sim(question,hop_3)\n",
    "\n",
    "        #     #         print('step3')\n",
    "        #     m=0\n",
    "        #     # print(kg[entity][relation])\n",
    "        #     # ['\"2006\"']\n",
    "        #     # ['Private_company_limited_by_shares']\n",
    "        #     # ['Warsaw']\n",
    "        #     # ['Poland']\n",
    "        #     # ['Poland']\n",
    "        #     # ['Adam_Kuriański', 'Andrzej_Chajec']\n",
    "        #     # ['\"Telecommunications\"', 'Telecommunication']\n",
    "        #     # ['Telecommunication']\n",
    "        #     # ['\"Aero 2\"']\n",
    "        #     # ['Adam_Kuriański', '\"\\'\\'\\'\"', 'Andrzej_Chajec']\n",
    "            \n",
    "        #     # print(kg[entity][relation])\n",
    "        #     # 2006\n",
    "        super_graph.append(r2[:2])\n",
    "\n",
    "        break\n",
    "        # calc_ent_channel(question,subgraph,n)\n",
    "        subgraph = [] \n",
    "        hop_2=[]\n",
    "        hop_3=[]\n",
    "    n+=1\n",
    "    # print(super_graph)\n",
    "    q_a_pairs[question]=super_graph\t\n",
    "    super_graph=[]\n",
    "            #     for obj in kg[entity][relation]:\n",
    "            #         print(kg[obj].items())\n",
    "            #         break \n",
    "                # break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_freq_sim(question,facts):\n",
    "    input_text = \"\"\n",
    "    question1=set(question)\n",
    "    # print(facts[0])\n",
    "    fact_list=[]\n",
    "    ans_list=[]\n",
    "    result=[]\n",
    "    for i in facts:\n",
    "        fc=literal_eval(i)\n",
    "        prom_text = set(fc[0]+fc[1])\n",
    "\n",
    "        ans_h = len(question1.intersection(prom_text))/((len(question1)))\n",
    "        ans_list.append(ans_h)\n",
    "        fact_list.append(fc[1])\n",
    "    index=np.argsort(ans_list)[::-1]  \n",
    "    for j in index:\n",
    "        result.append(fact_list[j])\n",
    "    step=min(5,len(ans_list))\n",
    "    return result[:step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cos_sim2(query, corpus):\n",
    "    c1=[]\n",
    "    for i in corpus:\n",
    "        i=str([i])\n",
    "        c1.append(i) \n",
    "    ranks = model2.rank(query, c1,return_documents=True)\n",
    "    stop=0\n",
    "    result=[]\n",
    "    scores=[]\n",
    "    for i in ranks:\n",
    "\n",
    "        triplet=literal_eval(i['text'])\n",
    "        result.append(triplet)\n",
    "        scores.append(i['score'])\n",
    "        # stop+=1\n",
    "        # if stop==5:\n",
    "        #     break\n",
    "\n",
    "    return(result, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'its orbital period of 1202.846 days; its epoch date is the 26th of November 2005.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m sub\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(qu)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 7\u001b[0m     e_list\u001b[38;5;241m=\u001b[39m\u001b[43mq_a_pairs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mqu\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(e_list)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      9\u001b[0m         sub\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'its orbital period of 1202.846 days; its epoch date is the 26th of November 2005.'"
     ]
    }
   ],
   "source": [
    "k1=[]\n",
    "hits_1=0\n",
    "hits_3=0\n",
    "hits_5=0\n",
    "sub=0\n",
    "for i in range(len(qu)-1):\n",
    "    e_list=q_a_pairs[qu[i]]\n",
    "    if len(e_list)==0:\n",
    "        sub+=1\n",
    "        continue\n",
    "    else:\n",
    "        ev_list=evidence_set_dict[i]\n",
    "        r2,scores=calc_cos_sim2(qu[i],list(e_list[0]))\n",
    "        flat_list = [item for sublist in list(e_list[0]) for item in sublist]\n",
    "        print(scores[0])\n",
    "        # r2=calc_freq_sim(qu[i],list(e_list[0]))\n",
    "        r2_top1=r2[0]\n",
    "        r2_top3=r2[:3]\n",
    "        if ev_list==r2_top1 or scores[0]>0.4:\n",
    "            hits_1+=1\n",
    "        if ev_list in flat_list:\n",
    "            hits_3+=1\n",
    "        if ev_list in flat_list:\n",
    "            hits_5+=1    \n",
    "         \n",
    "print(\"Accuracy for Hits@1 = \",hits_1/(1519-sub))\n",
    "print(\"Accuracy for Hits@3 = \",hits_3/(1519-sub))\n",
    "print(\"Accuracy for Hits@5 = \",hits_5/(1519-sub))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corpus_id': 3, 'score': np.float32(0.40590367), 'text': '[\\'Paul_Biya\\', \\'office\\', \\'\"2ndPresident of Cameroon\"\\']'}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m r2\u001b[38;5;241m=\u001b[39mcalc_cos_sim(qu[i],\u001b[38;5;28mlist\u001b[39m(e_list[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# r2=calc_freq_sim(qu[i],list(e_list[0]))\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m r2_top1\u001b[38;5;241m=\u001b[39m\u001b[43mr2\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     17\u001b[0m r2_top3\u001b[38;5;241m=\u001b[39mr2[:\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ev_list\u001b[38;5;241m==\u001b[39mr2_top1:\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# k1=[]\n",
    "# hits_1=0\n",
    "# hits_3=0\n",
    "# hits_5=0\n",
    "# sub=0\n",
    "# for i in range(len(qu)-1):\n",
    "#     e_list=q_a_pairs[qu[i]]\n",
    "#     if len(e_list)==0:\n",
    "#         sub+=1\n",
    "#         continue\n",
    "#     else:\n",
    "#         ev_list=evidence_set_dict[i]\n",
    "            \n",
    "#         r2=calc_cos_sim(qu[i],list(e_list[0]))\n",
    "#         # r2=calc_freq_sim(qu[i],list(e_list[0]))\n",
    "#         r2_top1=r2[0]\n",
    "#         r2_top3=r2[:3]\n",
    "#         if ev_list==r2_top1:\n",
    "#             hits_1+=1\n",
    "#         if ev_list in r2_top3:\n",
    "#             hits_3+=1\n",
    "#         if ev_list in r2:\n",
    "#             hits_5+=1    \n",
    "         \n",
    "# print(\"Accuracy for Hits@1 = \",hits_1/(1519-sub))\n",
    "# print(\"Accuracy for Hits@3 = \",hits_3/(1519-sub))\n",
    "# print(\"Accuracy for Hits@5 = \",hits_5/(1519-sub))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\", device_map = 'auto')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\", device_map = 'auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ans_cross_entroy(question_p,facts):\n",
    "    input_text = \"\"\n",
    "\n",
    "    prom_text = str(facts)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_ids = tokenizer.encode(prom_text, return_tensors=\"pt\").to(device)\n",
    "        beam0_logits = model(input_ids).logits[:, -1, :].to(device)\n",
    "        beam0_prob = torch.softmax(beam0_logits, dim=-1).to(device)\n",
    "        print(type(beam0_prob),type(question_p))\n",
    "        ans_h = (question_p * torch.log(beam0_prob/question_p)).sum().to(device)\n",
    "    if len(facts)>3:\n",
    "        ans_h=ans_h/2\n",
    "    return ans_h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def half_s(query):\n",
    "    with torch.no_grad():\n",
    "            input_ids = tokenizer.encode(query, return_tensors=\"pt\").to(device)\n",
    "            beam0_logits = model(input_ids).logits[:, -1, :].to(device)\n",
    "            beam0_prob = torch.softmax(beam0_logits, dim=-1).to(device)\n",
    "           \n",
    "    return beam0_prob      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ent(question,subgraph):\n",
    "    ent=[]\n",
    "    question=(str(question))\n",
    "    question_p=half_s(question)\n",
    "    print(question_p)\n",
    "    result=[]\n",
    "    facts_list=[]\n",
    "    for i in subgraph:\n",
    "        print(i)\n",
    "        facts_list.append(i)\n",
    "        ent.append(ans_cross_entroy(question_p,i).cpu().item())\n",
    "\n",
    "    index=np.argsort(ent)[::-1]  \n",
    "    for j in index:\n",
    "        result.append(facts_list[j])\n",
    "    step=min(5,len(ent))\n",
    "    return result[:step]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5438e-05, 0.0000e+00, 2.4438e-05,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]], device='cuda:0', dtype=torch.float16)\n",
      "['Paul_Biya', '~leader', 'Cameroon']\n",
      "<class 'torch.Tensor'> <class 'str'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'Tensor' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[150], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m     ev_list\u001b[38;5;241m=\u001b[39mevidence_set_dict[i]\n\u001b[1;32m---> 14\u001b[0m     r2\u001b[38;5;241m=\u001b[39m\u001b[43mcalc_ent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqu\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43me_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# r2=calc_freq_sim(qu[i],list(e_list[0]))\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     r2_top1\u001b[38;5;241m=\u001b[39mr2[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[149], line 11\u001b[0m, in \u001b[0;36mcalc_ent\u001b[1;34m(question, subgraph)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m     10\u001b[0m     facts_list\u001b[38;5;241m.\u001b[39mappend(i)\n\u001b[1;32m---> 11\u001b[0m     ent\u001b[38;5;241m.\u001b[39mappend(\u001b[43mans_cross_entroy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     13\u001b[0m index\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39margsort(ent)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m index:\n",
      "Cell \u001b[1;32mIn[147], line 11\u001b[0m, in \u001b[0;36mans_cross_entroy\u001b[1;34m(question_p, facts)\u001b[0m\n\u001b[0;32m      9\u001b[0m     beam0_prob \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(beam0_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(beam0_prob),\u001b[38;5;28mtype\u001b[39m(question_p))\n\u001b[1;32m---> 11\u001b[0m     ans_h \u001b[38;5;241m=\u001b[39m (question_p \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(\u001b[43mbeam0_prob\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mquestion_p\u001b[49m))\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(facts)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m     13\u001b[0m     ans_h\u001b[38;5;241m=\u001b[39mans_h\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'Tensor' and 'str'"
     ]
    }
   ],
   "source": [
    "k1=[]\n",
    "hits_1=0\n",
    "hits_3=0\n",
    "hits_5=0\n",
    "sub=0\n",
    "for i in range(len(qu)-1):\n",
    "    e_list=q_a_pairs[qu[i]]\n",
    "    if len(e_list)==0:\n",
    "        sub+=1\n",
    "        continue\n",
    "    else:\n",
    "        ev_list=evidence_set_dict[i]\n",
    "            \n",
    "        r2=calc_ent(qu[i],list(e_list[0]))\n",
    "        # r2=calc_freq_sim(qu[i],list(e_list[0]))\n",
    "        r2_top1=r2[0]\n",
    "        r2_top3=r2[:3]\n",
    "        if ev_list==r2_top1:\n",
    "            hits_1+=1\n",
    "        if ev_list in r2_top3:\n",
    "            hits_3+=1\n",
    "        if ev_list in r2:\n",
    "            hits_5+=1    \n",
    "         \n",
    "print(\"Accuracy for Hits@1 = \",hits_1/(1519-sub))\n",
    "print(\"Accuracy for Hits@3 = \",hits_3/(1519-sub))\n",
    "print(\"Accuracy for Hits@5 = \",hits_5/(1519-sub))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
